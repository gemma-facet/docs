---
title: Getting Started with Facet AI
---

Facet AI is a no-code web platform where you can easily fine tune the Gemma family of vision and small language models for your task and domain specific adaptations, without worrying about dataset formatting, cloud engineering, training algorithms — you can get started from scratch in a few minutes with little programming & AI experience!

## Features and Limitations

### What we support

- Creating an account to manage your datasets, training, evaluation, etc.

- Augmentation and preparation of datasets from custom upload or Hugging Face Hub for language modelling, preference tuning, reinforcement learning, and multimodal tasks

- Configuring fine tuning for Gemma 3 270M, 1B, 4B, 12B, 27B, Gemma 3n E2B and E4B using supervised fine tuning (SFT), direct policy optimization (DPO), and group-related policy optimization (GRPO).

- Run inference & evaluation suites on fine tuned models to assess their performance

- Export models in various formats (adapter, merged, GGUF) and quantizations and deploy them easily on Google Cloud Run or other providers with pre-built containers

### What we're not for

- Dataset collection: while we help you augment your dataset based on uploaded sources, we do not e.g. scrape the web to collect data

- LLM deployment: our service will not run the deployed model for you; you should set up your own runtime / cloud project to do that — we have a tutorial for it.

## Getting Started

You can access our service in three common ways:

1. Directly through our web interface (runs on our managed Google Cloud Platform). Create an account and access the app from the project website.

2. Deploy the project on your own cloud service provider (recommended: GCP). Follow the "deploy your own service" guide. Our docker container images are served on Docker Hub. We provide Terraform to help set up the GCP project; for other providers (e.g. AWS) you can use the pre-built containers.

3. Fork the project for your own customization. We're open-source — see `CONTRIBUTING.md` for contribution details.

## Usage guide

If you have experience fine tuning LLMs, the UI should be intuitive. If you're confused, refer to these docs:

- Creating datasets — /dataset-preprocessing/datasets
- Creating fine-tune jobs — /fine-tuning/training
- Inference, evaluation, export — /evaluation/inference_export
- Deploying the model — /deployment/deployment

## Acknowledgements

We are grateful for Hugging Face `datasets`, `transformers`, `trl`, and Unsloth `unsloth, unsloth-zoo` as the underlying backend for fine tuning, and `vLLM, llama.cpp, ollama` as inference backends.

We also thank mentors at Google Cloud, Google DeepMind, and Google Summer of Code for their support.

---

import { Callout } from "mint-ui";

<Callout title="Developer docs" type="info">
  This guide is for platform users. For implementation details or to contribute,
  see the `README.md` in each subfolder.
</Callout>
