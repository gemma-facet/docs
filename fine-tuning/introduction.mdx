---
title: "Fine-tuning Gemma Models"
description: "Learn how to fine-tune Gemma models for your specific tasks using Facet's training service"
---

Fine‑tune a Gemma model on your domain data with a simple, repeatable flow. You’ll pick a base model, choose a training method (SFT, DPO/ORPO, or GRPO), and run efficiently with Full, LoRA, or QLoRA depending on resources. Vision variants support text+image.

<Steps>
<Step title="Prepare your dataset">
  Process data into conversation or preference format using the [dataset preprocessing guide](/dataset-preprocessing/introduction).
  <Check>
    Verify a small sample looks correct before training.
  </Check>
</Step>
<Step title="Select base model and method">
  Pick a Gemma size that fits your budget, then choose SFT (supervised), DPO/ORPO (preference), or GRPO (reasoning with rewards).
</Step>
<Step title="Choose efficiency">
  Start with QLoRA for strong results on modest hardware; use Full finetune only when you need maximal capacity.
</Step>
<Step title="Launch and monitor">
  Start the job and watch training/validation signals to catch issues early.
</Step>
<Step title="Evaluate and iterate">
  Test the model, compare to the baseline, and iterate on data or settings.
</Step>
</Steps>

Next: open the [Training guide](/fine-tuning/training) for step‑by‑step configuration details.
