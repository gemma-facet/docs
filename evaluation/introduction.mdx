---
title: "Eval and Exports Workflow"
description: "Test your fine-tuned models and run comprehensive evaluations to assess performance"
---

Evaluate a trained model with quick, interactive inference or run a batch job against a dataset to get objective metrics. Review results, compare to a baseline, and export the best checkpoint for deployment.

<Steps>
<Step title="Select a model">
  Pick a completed training job or a previously exported model.
  <Check>
    Have the model source path and base model ID handy.
  </Check>
</Step>
<Step title="Choose evaluation mode">
  Start with single‑prompt inference to sanity‑check behavior, then move to batch evaluation on a labeled dataset.
</Step>
<Step title="Pick metrics">
  Select task‑appropriate metrics (e.g., BERTScore/ROUGE for generation, EM/F1 for QA, accuracy/F1 for classification).
</Step>
<Step title="Run and monitor">
  Launch the job and watch progress; large datasets take longer.
</Step>
<Step title="Review and compare">
  Inspect metrics and samples, compare against a baseline, and note failure patterns to guide the next training round.
</Step>
<Step title="Export for serving">
  When satisfied, export the model in your preferred format for deployment.
</Step>
</Steps>

Next: see the [Inference & Export guide](/evaluation/inference_export) for detailed setup and options.
